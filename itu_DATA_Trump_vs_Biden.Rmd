---
title: "ITU Project"
# output: html_notebook
# editor_options: 
  # chunk_output_type: inline
---

```{r Twitter Social Media Analytics - Trump vs. Biden Project}
```


```{r Packages}


library(tm)
library(qdap) 
library(wordcloud)
library(viridisLite)
library(plotrix)
library(qdap)
library(tidyverse)
library(tidytext)
library(magrittr)
library(metricsgraphics)
library(ggthemes)
library(dplyr)
library(ggplot2)
library(tibbletime)
library(topicmodels)
library(furrr)
library(stm)

install.packages("furrr")
install.packages("tibbletime")
install.packages("topicmodels")
install.packages("stm")



head(biden_cptw)
head(trump_cptw)


```


```{r Importing Data}

#Import Data from CSV
biden_tw <- read.csv("./archive/JoeBidenTweets.csv")
trump_tw <- read.csv("./archive/realdonaldtrump.csv")
trump_tw1 <- read.csv("./archive/trumptweets.csv")
```

```{r Cleaning Data}
# Isolate text from tweets

biden_cptw <- filter(biden_tw, timestamp >= "2019-04-25 10:00")
View(biden_cptw)

biden_tweets <- biden_cptw$tweet

# Make a vector source from biden_tweets
biden_source <- VectorSource(biden_tweets)

# Make a volatile corpus: biden_corpus
biden_corpus <- VCorpus(biden_source)
biden_corpus[[35]][1]


#We define a cleaning function
clean_corpus <- function(corpus){
  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(strip), char.keep="#")
  corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, stripWhitespace)
  return(corpus)
}

biden_corpuscl <- clean_corpus(biden_corpus)
biden_corpuscl[[35]][1]

# Create a Term Document Matrix out of our cleaned data and convert it to a Matrix

biden_tdm <- TermDocumentMatrix(biden_corpuscl)
View(biden_tdm)
dim(biden_tdm)


# Convert biden_tdm to a matrix
biden_m <- as.matrix(biden_tdm)
biden_m

# Create a Document Term Matrix out of our cleaned data and convert it to a Matrix
biden_dtm <- DocumentTermMatrix(biden_corpuscl)

# Convert biden_dtm to a matrix
biden_m2 <- as.matrix(biden_dtm)

# Trump data

View(trump_tw)

trump_cptw <- filter(trump_tw, date >= "2019-06-18 00:10:06")

# Isolate chardonnay text from tweets
trump_tweets <- trump_cptw$content
View(trump_tweets)

# Make a vector source from chardonnay_tweets
trump_source <- VectorSource(trump_tweets)

# Make a volatile corpus: chardonnay_corpus
trump_corpus <- VCorpus(trump_source)
trump_corpus[[45]][1]

trump_corpuscl <- clean_corpus(trump_corpus)

trump_corpuscl[[90]][1]
View(trump_corpuscl)

# Create a Term Document Matrix out of our cleaned data and convert it to a Matrix
trump_tdm <- TermDocumentMatrix(trump_corpuscl[1:10])
View(trump_tdm)
dim(trump_tdm)


# Convert trump_tdm to a matrix
trump_m <- as.matrix(trump_tdm)
View(trump_tdm)

```

```{r Simple Sentiment Analysis - Biden Polarity Score}
pos_score <- polarity(biden_tweets)

head(biden_cptw)


# Get counts
pos_counts <- counts(pos_score)

pos_counts


# Number of positive words
n_good <- length(pos_counts$pos.words[[1]]) 

n_good

# Total number of words
n_words <- pos_counts$wc

# Verify polarity score
n_good / sqrt(n_words)
```


```{r Sentiment Analysis with Afinn}
# Tidy up the DTM
biden_tidy <- tidy(biden_dtm)

# Examine tidy with a word you saw
biden_tidy[831:835, ]


#Get Sentiment Lexicons
afinn <- get_sentiments("afinn")

afinn %>%
	summarize(
		min = min(value),
		max = max(value)
)

head(afinn)

# Count AFINN scores
afinn %>% 
  count(value)

# Join text to lexicon (afinn)

str(biden_tidy)

biden_afinn_words <- inner_join(biden_tidy, afinn, by = c("term" = "word"))


# Examine
biden_afinn_words

# Get counts by sentiment
biden_afinn_words %>%
  count(value)

  
# Create a column index, equal to as.numeric() applied to document. This occurs within mutate() in the tidyverse.
biden_afinn_words <- biden_afinn_words %>%
  # Set index to numeric document
  mutate(index = as.numeric(document))
  
biden_afinn_words

# Create biden_count_afinn by forwarding ag_afinn_words to count(), passing in value, index. Gives us value by twet
biden_count_afinn <- biden_afinn_words %>%
  # Count by value, index
  count(value, index)

biden_count_afinn


# Generate biden_spread_afinn by piping ag_count to spread() which contains sentiment, n, and fill = 0.
biden_spread_afinn <- biden_count_afinn %>%
  # Spread sentiments
  spread(value, n, fill = 0)

# Review the spread data
biden_spread_afinn

head(biden_afinn_words,20)

# exclude variables v1, v2, v3
biden_polarity_plot <- biden_afinn_words %>%
                    select(term, count, index)

head(biden_polarity_plot_afinn,10)

biden_polarity_afinn <- biden_polarity_plot %>%
  # Inner join to lexicon
  inner_join(afinn, by = c("term" = "word")) %>%
  # Count the sentiment scores
  count(value, index) %>% 
  # Spread the sentiment into positive and negative columns
  spread(value, n, fill = 0) %>%
  # Add polarity column
  mutate(polarity = positive - negative)

biden_polarity_afinn

# Plot polarity vs. index
ggplot(biden_polarity, aes(x = index, y = polarity)) + 
  # Add a smooth trend curve
  geom_smooth()


# Create a Document Term Matrix out of our cleaned data and convert it to a Matrix
trump_dtm <- DocumentTermMatrix(trump_corpuscl)

# Convert biden_dtm to a matrix
trump_m2 <- as.matrix(biden_dtm)

# Tidy up the DTM
trump_tidy <- tidy(trump_dtm)


trump_afinn_words <- trump_tidy %>% 
	inner_join(afinn, by = c("term" = "word"))

# Combine. Use bind_rows() to combine ag_afinn to oz_afinn. Set the .id argument to "book" to create a new column with the name of each book.
all_df_afinn <- bind_rows(biden = biden_afinn_words, 
                          trump = trump_afinn_words, 
                          .id = "Tweets")




# Plot value, filled by Tweets
ggplot(all_df_afinn, aes(value, fill = Tweets)) + 
  # Set the alpha transparency to 0.3.
  geom_density(alpha = 0.3) + 
  theme_gdocs() +
  ggtitle("AFINN Score Densities")


biden_afinn_words



biden_afinn_agg <- biden_afinn_words %>% 
  # Group by tweet
  select(value, index, count) %>%
  group_by(index)


# To see the value of eact tweet we code:
 biden_afinn_agg_pl <- biden_afinn_agg %>%
  # Sum values times n (by tweet)
  summarize(total_value = sum(value * count))

# Plot total_value vs. line
ggplot(biden_afinn_agg_pl, aes(x = index, y = total_value)) + 
  # Add a smooth trend curve
  geom_smooth() + ggtitle("Plot of AFINN by Each Tweet") + 
  xlab("Tweet Number") + 
  ylab("AFINN Value")
```


```{r Sentiment Analysis with Afinn}

# Start with the tidied Twitter data
as.data.farbiden_tweets %>% 
  # Count each word used in each tweet
  count(word, tweet_id) %>%
  # Use the word counts by tweet to create a DTM
  cast_dtm(tweet_id, word, n)


head(biden_tweets)

#{r Get Sentiment Score between -5 and 5}
# Tidy up the DTM
biden_tidy <- tidy(biden_dtm)

#Get Sentiment Lexicons
afinn <- get_sentiments("afinn")

# Join two data frames
biden_afinn_words <- inner_join(biden_tidy, afinn, by = c("term" = "word"))

# Create a Document Term Matrix out of our cleaned data and convert it to a Matrix
trump_dtm <- DocumentTermMatrix(trump_corpuscl)

# Tidy up the DTM
trump_tidy <- tidy(trump_dtm)


trump_afinn_words <- trump_tidy %>% 
  inner_join(afinn, by = c("term" = "word"))

# Combine. Use bind_rows() to combine ag_afinn to oz_afinn. Set the .id argument to "book" to create a new column with the name of each book.
all_df_afinn <- bind_rows(biden = biden_afinn_words, 
                          trump = trump_afinn_words, 
                          .id = "Tweets")



#######

# Plot value, filled by Tweets
ggplot(all_df_afinn, aes(value, fill = Tweets)) + 
  # Set the alpha transparency to 0.3.
  geom_density(alpha = 0.3) + 
  theme_gdocs() +
  ggtitle("AFINN Score Densities")

#######
    
all_afinn_agg <- all_df_afinn %>% 
  # Group by tweet
    select(value, document, count) %>%
    group_by(document) %>% 
    summarize(total_value = sum(value * count))   

biden_afinn_agg <- biden_afinn_words %>% 
  # Group by tweet
    select(value, document, count) %>%
    group_by(document) %>% 
    summarize(total_value = sum(value * count)) 

trump_afinn_agg <- trump_afinn_words %>% 
  # Group by tweet
    select(value, document, count) %>%
    group_by(document) %>% 
    summarize(total_value = sum(value * count)) 

mean(biden_afinn_agg$total_value)


trump_cptw$document <- seq.int(nrow(trump_cptw))
biden_cptw$document <- seq.int(nrow(biden_cptw))

trump_afinn_agg$document <- as.numeric(trump_afinn_agg$document)
biden_afinn_agg$document <- as.numeric(biden_afinn_agg$document)


trump_cptw_score <- inner_join(trump_cptw, trump_afinn_agg, by = "document")
biden_cptw_score <- inner_join(biden_cptw, trump_afinn_agg, by = "document")


head(biden_cptw)
head(trump_cptw)

all_cptw <- bind_rows(biden=biden_cptw_score, trump=trump_cptw_score, .id = "candidates") %>% 
            unite(date, timestamp, date, na.rm = T) %>% 
            unite(favs, likes, favorites, na.rm = T) %>% 
            unite(twits, tweet, content, na.rm = T) %>% 
            unite(url, url, link, na.rm = T) %>% 
            unite(mentions, mentions, replies, na.rm = T)


sapply(all_cptw, function(x) sum(is.na(x)))

table(all_cptw$candidates)


```


```{r Sentiment Analysis with Tidy - Qdap Polarity - Trump & Biden}

# Qdap polarity

biden_qdap_polarity <- polarity(biden_tweets)
trump_qdap_polarity <- polarity(trump_tweets)

biden_tw_df_qdap <- as.data.frame(biden_tweets)
trump_tw_df_qdap <- as.data.frame(trump_tweets)

# Combine. Use bind_rows() to combine biden_qdap_polarity to trump_qdap_polarity. Set the .id argument to "Tweets" to create a new column with the name of each book.
colnames(biden_tw_df_qdap) <- c("Tweet Content")
colnames(trump_tw_df_qdap) <-  c("Tweet Content")
 

all_df_qdap <- bind_rows(biden = biden_tw_df_qdap, 
                         trump = trump_tw_df_qdap,
                         .id = "Candidate")

all_df_qdap


# Calc overall polarity score
all_df_qdap %$% polarity(`Tweet Content`)


# Calc polarity score by person. This will calculate polarity according to each individual person. Since it is all within parentheses the result will be printed too.
(polarity_qdap_canditates <- all_df_qdap %$% polarity(`Tweet Content`, Candidate))

# Counts table from polarity_qdap_canditates. Apply counts() to polarity_qdap_canditates to print the specific emotional words that were found.
counts(polarity_qdap_canditates)

# Plot the conversation polarity
plot(polarity_qdap_canditates)



```


```{r Sentiment Analysis with Bing}

# Get Bing lexicon
bing <- get_sentiments("bing")

head(bing)

# Join text to lexicon (bing)
str(biden_tidy)

biden_bing_words <- inner_join(biden_tidy, bing, by = c("term" = "word"))

# Examine
biden_bing_words

# Get counts by sentiment
biden_bing_words %>%
  count(sentiment)
  
# Create a column index, equal to as.numeric() applied to document. This occurs within mutate() in the tidyverse.
biden_bing_words <- biden_bing_words %>%
  # Set index to numeric document
  mutate(index = as.numeric(document))
  
head(biden_bing_words,10)

# Create ag_count by forwarding ag_bing_words to count(), passing in sentiment, index.
biden_count <- biden_bing_words %>%
  # Count by sentiment, index
  count(sentiment, index)

biden_count


# Generate moby_spread by piping ag_count to spread() which contains sentiment, n, and fill = 0.
biden_spread <- biden_count %>%
  # Spread sentiments
  spread(sentiment, n, fill = 0)

# Review the spread data
biden_spread

head(biden_bing_words)

# exclude variables v1, v2, v3
biden_polarity_plot <- biden_bing_words %>%
                    select(term, count, index)

str(biden_polarity_plot)

biden_polarity <- biden_polarity_plot %>%
  # Inner join to lexicon
  inner_join(bing, by = c("term" = "word")) %>%
  # Count the sentiment scores
  count(sentiment, index) %>% 
  # Spread the sentiment into positive and negative columns
  spread(sentiment, n, fill = 0) %>%
  # Add polarity column
  mutate(polarity = positive - negative)

biden_polarity

# Plot polarity vs. index
ggplot(biden_polarity, aes(x = index, y = polarity)) + 
  # Add a smooth trend curve
  geom_smooth()



```

```{r Descriptive Analysis}
# Dimensions of DTM matrix
dim(biden_dtm)

# Calculate the row sums of biden_m
term_frequency <- rowSums(biden_m)
View(term_frequency)

# Sort term_frequency in decreasing order
term_frequency <- sort(term_frequency, decreasing = TRUE)
View(term_frequency)

# View the top 10 most common words
term_frequency[1:10]

# Plot a barchart of the 10 most common words
barplot(term_frequency[1:10], col = "tan", las = 2)

```


```{r Word Cloud}
# Vector of terms
terms_vec <- names(term_frequency)
View(terms_vec)

# Create a wordcloud for the values in word_freqs
wordcloud(terms_vec, term_frequency, 
          max.words = 50, colors = "red")
```


```{r Other Clouds and Networks}

```


```{r Other Clouds and Networks - 2}


# Analyzing Common Words btw Trump & Biden with commonality cloud =================================================
# Create all_coffee
all_trump <- paste(trump_tweets, collapse = " ")

# Create all_chardonnay
all_biden <- paste(biden_tweets, collapse = " ")

# Create all_tweets
all_tweets <- c(all_trump, all_biden)
View(all_tweets)

# Convert to a vector source
all_tweets <- VectorSource(all_tweets)
View(all_tweets)

# Create all_corpus
all_corpus <- VCorpus(all_tweets)

# Clean the corpus
all_clean <- clean_corpus(all_corpus)
content(all_clean[[5]])


# Create all_tdm
all_tdm <- TermDocumentMatrix(all_clean)
View(all_tdm)

# Create all_m
all_m <- as.matrix(all_tdm)
View(all_m)

# Print a commonality cloud
commonality.cloud(all_m, max.words = 100, colors = "steelblue1")

# Comparison Cloud =================================================

# Clean the corpus
all_clean <- clean_corpus(all_corpus)

# Create all_tdm
all_tdm <- TermDocumentMatrix(all_clean)

# Give the columns distinct names
colnames(all_tdm) <- c("Trump", "Biden")

# Create all_m
all_m <- as.matrix(all_tdm)
View(all_m)

# Create comparison cloud
comparison.cloud(all_m, colors = c("red", "blue"), max.words = 50)

```

```{r Pyramit Plot}
# Pyramit Plot =================================================

top25_df <- all_m %>%
# Convert to data frame
as_tibble(rownames = "word") %>%
# Keep rows where word appears everywhere
filter_all(all_vars(. > 0)) %>% 
# Get difference in counts
mutate(difference = Biden - Trump) %>%
# Keep rows with biggest difference
top_n(25, wt = difference) %>% 
# Arrange by descending difference
arrange(desc(difference))

View(top25_df)

# Draw the Pyramid Plot

pyramid.plot(
# Chardonnay counts
  top25_df$Biden,
# Coffee counts
  top25_df$Trump,
  # gap = 1,
# Words
  labels = top25_df$word,
  top.labels = c("Biden", "Words", "Trump"),
  main = "Words in Common",
  unit = NULL,
  gap = 8,
)
```


```{r Word Network}

# Word Networks =================================================

# Add “donald” and “president” to the list: new_stops
new_stops <- c("donald", "my", "along", "and", "president", stopwords("en"))

# Remove stop words from text
biden_tweets_nt <- removeWords(biden_tweets, new_stops)


# Word association
word_associate(biden_tweets_nt[1:1000], match.string = "Trump", 
               network.plot = TRUE, cloud.colors = c("gray85", "darkred"))

# Add title
title(main = "Trump in Biden Tweet Associations")

```


```{r Topic Modelling}


biden_tidy1 <- biden_tidy %>%
  filter(!term %in% c("the", "get", "just", "can", "don't", "stil", "it's", "its", "this", "and", "'re", "I'm", "many", "i'm","'re ", " 're", "cant", "can't", "i'm ", "i'll", "can't", "it's ", "but", "let"))

# Start with the tidied Twitter data
biden_tidy_dtm <- biden_tidy1 %>% 
  # Count each word used in each tweet
  count(term, document) %>%
  # Use the word counts by tweet to create a DTM
  cast_sparse(document, term, n)

# Coerce dtm_twitter into a matrix called matrix_twitter
matrix_biden_topic <- as.matrix(biden_tidy_dtm)

# Print rows 1 through 5 and columns 90 through 95
matrix_biden_topic[1:5, 90:95]


# Run an LDA with 2 topics and a Gibbs sampler
lda_out_biden6 <- LDA(
	biden_tidy_dtm, 
	k = 6,
	method = "Gibbs",
	control = list(seed = 1200)
)


# Tidy the matrix of word probabilities
lda_topics_biden6 <- lda_out_biden5 %>%
tidy(matrix = "beta")

# Arrange the topics by word probabilities in descending order___ %>% 
 lda_topics_biden6 %>%
arrange(desc(beta))

 
biden_documents <- tidy(lda_out_biden6, matrix = "gamma") %>% arrange(desc(gamma))
biden_documents
 

# Select the top 15 terms by topic and reorder term
word_probs_biden6 <- lda_topics_biden5 %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
mutate(term6 = fct_reorder(term, beta))



ggplot(
  word_probs_biden6, 
  aes(term6, beta, fill = as.factor(topic))
) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip()


word_probs_biden8$topic <- factor(word_probs_biden8$topic, levels = c("1", "2", "3", "4", "5", "6", "7", "8", "9"),
                  labels = c("Nation", "Family - Middle Class", "Health Care", "Democrats", "Working Class", "Campaing", "Violence", "Trump Presidency", "Voter Turnout"))




```

