xaringan:::inf_mr()
options(htmltools.dir.version = FALSE)
library(tm)
library(qdap)
library(wordcloud)
library(viridisLite)
library(plotrix)
library(qdap)
library(tidyverse)
library(tidytext)
library(magrittr)
library(metricsgraphics)
library(ggthemes)
library(dplyr)
library(ggplot2)
library(tibbletime)
library(topicmodels)
library(furrr)
library(stm)
options(htmltools.dir.version = FALSE)
library(tm)
library(qdap)
library(wordcloud)
library(viridisLite)
library(plotrix)
library(qdap)
library(tidyverse)
library(tidytext)
library(magrittr)
library(metricsgraphics)
library(ggthemes)
library(dplyr)
library(ggplot2)
library(tibbletime)
library(topicmodels)
library(furrr)
library(stm)
biden_tw <- read.csv("./archive/JoeBidenTweets.csv")
biden_tw <- read.csv("../archive/JoeBidenTweets.csv")
trump_tw <- read.csv("../archive/realdonaldtrump.csv")
biden_cptw <- filter(biden_tw, timestamp >= "2019-04-25 10:00")
trump_cptw <- filter(trump_tw, date >= "2019-06-18 00:10:06")
clean_corpus <- function(corpus){
corpus <- tm_map(corpus, content_transformer(replace_abbreviation))
corpus <- tm_map(corpus, removeNumbers)
corpus <- tm_map(corpus, content_transformer(strip), char.keep="#")
corpus <- tm_map(corpus, removeWords, c(stopwords("en")))
corpus <- tm_map(corpus, content_transformer(tolower))
corpus <- tm_map(corpus, stripWhitespace)
return(corpus)
}
biden_tweets <- biden_cptw$tweet
trump_tweets <- trump_cptw$content
# Make a vector source
biden_source <- VectorSource(biden_tweets)
trump_source <- VectorSource(trump_tweets)
biden_corpus <- VCorpus(biden_source)
trump_corpus <- VCorpus(trump_source)
# Create a Term Document Matrix out of our cleaned data and convert it to a Matrix
biden_corpuscl <- clean_corpus(biden_corpus)
trump_corpuscl <- clean_corpus(trump_corpus)
biden_tdm <- TermDocumentMatrix(biden_corpuscl)
trump_tdm <- TermDocumentMatrix(trump_corpuscl)
biden_m <- as.matrix(biden_tdm)
trump_m <- as.matrix(trump_tdm)
biden_dtm <- DocumentTermMatrix(biden_corpuscl)
trump_dtm <- DocumentTermMatrix(trump_corpuscl)
biden_m2 <- as.matrix(biden_dtm)
trump_m2 <- as.matrix(trump_dtm)
options(htmltools.dir.version = FALSE)
# Calculate the row sums of biden_m
term_frequency_biden <- rowSums(biden_m)
term_frequency_trump <- rowSums(trump_m)
# Sort term_frequency in decreasing order
term_frequency_biden <- sort(term_frequency, decreasing = TRUE)
# Calculate the row sums of biden_m
term_frequency_biden <- rowSums(biden_m)
term_frequency_trump <- rowSums(trump_m)
# Sort term_frequency in decreasing order
term_frequency_biden <- sort(term_frequency_biden, decreasing = TRUE)
term_frequency_trump <- sort(term_frequency_trump, decreasing = TRUE)
# Plot a barchart of the 10 most common words
barplot(term_frequency_biden[1:10], col = "tan", las = 2)
# Plot a barchart of the 10 most common words
barplot(term_frequency_trump[1:10], col = "tan", las = 2)
knit()
knitr::knit()
knitr::knit("./sunum/trump_biden_presentation.Rmd")
knitr::knit("./sunum/trump_biden_presentation.Rmd")
library(knitr)
knit("./sunum/trump_biden_presentation.Rmd")
knitr::knit("trump_biden_presentation.Rmd")
rmarkdown::render()
rmarkdown::render("trump_biden_presentation.Rmd")
getwd()
setwd("C:/Users/ysf/Desktop/itu_project/sunum")
knitr::knit("trump_biden_presentation.Rmd")
rmarkdown::render("trump_biden_presentation.Rmd")
knit2html("trump_biden_presentation.Rmd")
rmarkdown::render("trump_biden_presentation.Rmd")
rmarkdown::render("trump_biden_presentation.Rmd")
rmarkdown::render("trump_biden_presentation.Rmd")
# Vector of terms
terms_vec_trump <- names(term_frequency_trump)
# Create a wordcloud for the values in word_freqs
wordcloud(terms_vec_trump, term_frequency_trump,
max.words = 50, colors = "red")
# Vector of terms
terms_vec_biden <- names(term_frequency_biden)
# Vector of terms
terms_vec_trump <- names(term_frequency_trump)
# Create a wordcloud for the values in word_freqs
wordcloud(terms_vec_trump, term_frequency_trump,
max.words = 50, colors = "red")
# Vector of terms
terms_vec_biden <- names(term_frequency_biden)
View(terms_vec_biden)
# Create a wordcloud for the values in word_freqs
wordcloud(terms_vec_biden, term_frequency_biden,
max.words = 50, colors = "blue")
# Vector of terms
terms_vec_biden <- names(term_frequency_biden)
View(terms_vec_biden)
# Create a wordcloud for the values in word_freqs
wordcloud(terms_vec_biden, term_frequency_biden,
max.words = 50, colors = "blue")
# Analyzing Common Words btw Trump & Biden with commonality cloud
all_trump <- paste(trump_tweets, collapse = " ")
all_biden <- paste(biden_tweets, collapse = " ")
# Create all_tweets
all_tweets <- c(all_trump, all_biden)
# Convert to a vector source
all_tweets <- VectorSource(all_tweets)
# Create all_corpus
all_corpus <- VCorpus(all_tweets)
# Clean the corpus
all_clean <- clean_corpus(all_corpus)
# Create all_tdm
all_tdm <- TermDocumentMatrix(all_clean)
# Create all_m
all_m <- as.matrix(all_tdm)
# Print a commonality cloud
commonality.cloud(all_m, max.words = 100, colors = "steelblue1")
# Comparison Cloud =================================================
# Give the columns distinct names
colnames(all_tdm) <- c("Trump", "Biden")
# Create all_m
all_m <- as.matrix(all_tdm)
View(all_m)
# Create comparison cloud
comparison.cloud(all_m, colors = c("red", "blue"), max.words = 50)
# Print a commonality cloud
commonality.cloud(all_m, max.words = 100, colors = "steelblue1")
# Give the columns distinct names
colnames(all_tdm) <- c("Trump", "Biden")
# Create all_m
all_m <- as.matrix(all_tdm)
# Create comparison cloud
comparison.cloud(all_m, colors = c("red", "blue"), max.words = 50)
rmarkdown::render("trump_biden_presentation.Rmd")
rmarkdown::render("trump_biden_presentation.Rmd")
rmarkdown::render("trump_biden_presentation.Rmd")
top25_df <- all_m %>%
# Convert to data frame
as_tibble(rownames = "word") %>%
# Keep rows where word appears everywhere
filter_all(all_vars(. > 0)) %>%
# Get difference in counts
mutate(difference = Biden - Trump) %>%
# Keep rows with biggest difference
top_n(25, wt = difference) %>%
# Arrange by descending difference
arrange(desc(difference))
```
top25_df <- all_m %>%
# Convert to data frame
as_tibble(rownames = "word") %>%
# Keep rows where word appears everywhere
filter_all(all_vars(. > 0)) %>%
# Get difference in counts
mutate(difference = Biden - Trump) %>%
# Keep rows with biggest difference
top_n(25, wt = difference) %>%
# Arrange by descending difference
arrange(desc(difference))
```
top25_df <- all_m %>%
# Convert to data frame
as_tibble(rownames = "word") %>%
# Keep rows where word appears everywhere
filter_all(all_vars(. > 0)) %>%
# Get difference in counts
mutate(difference = Biden - Trump) %>%
# Keep rows with biggest difference
top_n(25, wt = difference) %>%
# Arrange by descending difference
arrange(desc(difference))
top25_df <- all_m %>%
# Convert to data frame
as_tibble(rownames = "word") %>%
# Keep rows where word appears everywhere
filter_all(all_vars(. > 0)) %>%
# Get difference in counts
mutate(difference = Biden - Trump) %>%
# Keep rows with biggest difference
top_n(25, wt = difference) %>%
# Arrange by descending difference
arrange(desc(difference))
# Draw the Pyramid Plot
pyramid.plot(
# Chardonnay counts
top25_df$Biden,
# Coffee counts
top25_df$Trump,
# gap = 1,
# Words
labels = top25_df$word,
top.labels = c("Biden", "Words", "Trump"),
main = "Words in Common",
unit = NULL,
gap = 8,
)
rmarkdown::render("trump_biden_presentation.Rmd")
# Add “donald” and “president” to the list: new_stops
new_stops <- c("donald", "my", "along", "and", "president", stopwords("en"))
# Remove stop words from text
biden_tweets_nt <- removeWords(biden_tweets, new_stops)
# Word association
word_associate(biden_tweets_nt[1:999], match.string = "Trump",
network.plot = TRUE, cloud.colors = c("gray85", "darkred"))
# Add title
title(main = "Trump in Biden Tweet Associations")
rmarkdown::render("trump_biden_presentation.Rmd")
pos_score <- polarity(biden_tweets)
# Qdap polarity
biden_qdap_polarity <- polarity(biden_tweets)
trump_qdap_polarity <- polarity(trump_tweets)
biden_tw_df_qdap <- as.data.frame(biden_tweets)
trump_tw_df_qdap <- as.data.frame(trump_tweets)
# Combine. Use bind_rows() to combine biden_qdap_polarity to trump_qdap_polarity. Set the .id argument to "Tweets" to create a new column with the name of each book.
colnames(biden_tw_df_qdap) <- c("Tweet Content")
colnames(trump_tw_df_qdap) <-  c("Tweet Content")
all_df_qdap <- bind_rows(biden = biden_tw_df_qdap,
trump = trump_tw_df_qdap,
.id = "Candidate")
# Calc overall polarity score
all_df_qdap %$% polarity(`Tweet Content`)
# Tidy up the DTM
biden_tidy <- tidy(biden_dtm)
trump_tidy <- tidy(trump_dtm)
rmarkdown::render("trump_biden_presentation.Rmd")
rmarkdown::render("trump_biden_presentation.Rmd")
rmarkdown::render("trump_biden_presentation.Rmd")
#Get Sentiment Lexicons
afinn <- get_sentiments("afinn")
afinn %>%
summarize(
min = min(value),
max = max(value)
)
# Create a column index, equal to as.numeric() applied to document. This occurs within mutate() in the tidyverse.
biden_afinn_words <- biden_afinn_words %>%
# Set index to numeric document
mutate(index = as.numeric(document))
#Get Sentiment Lexicons
afinn <- get_sentiments("afinn")
# Tidy up the DTM
biden_tidy <- tidy(biden_dtm)
trump_tidy <- tidy(trump_dtm)
biden_afinn_words <- inner_join(biden_tidy, afinn, by = c("term" = "word"))
trump_afinn_words <- inner_join(trump_tidy, afinn, by = c("term" = "word"))
# Get counts by sentiment
biden_afinn_words %>%
count(value)
# Get counts by sentiment
trump_afinn_words %>%
count(value)
```
# Get counts by sentiment
trump_afinn_words %>%
count(value)
```
# Create a column index, equal to as.numeric() applied to document. This occurs within mutate() in the tidyverse.
biden_afinn_words <- biden_afinn_words %>%
# Set index to numeric document
mutate(index = as.numeric(document))
# Create biden_count_afinn by forwarding ag_afinn_words to count(), passing in value, index. Gives us value by twet
biden_count_afinn <- biden_afinn_words %>%
# Count by value, index
count(value, index)
# Generate biden_spread_afinn by piping ag_count to spread() which contains sentiment, n, and fill = 0.
biden_spread_afinn <- biden_count_afinn %>%
# Spread sentiments
spread(value, n, fill = 0)
# Create a column index, equal to as.numeric() applied to document. This occurs within mutate() in the tidyverse.
trump_afinn_words <- trump_afinn_words %>%
# Set index to numeric document
mutate(index = as.numeric(document))
# Create biden_count_afinn by forwarding ag_afinn_words to count(), passing in value, index. Gives us value by twet
trump_count_afinn <- trump_afinn_words %>%
# Count by value, index
count(value, index)
# Generate biden_spread_afinn by piping ag_count to spread() which contains sentiment, n, and fill = 0.
trump_spread_afinn <- trump_count_afinn %>%
# Spread sentiments
spread(value, n, fill = 0)
biden_polarity_plot <- biden_afinn_words %>%
select(term, count, index)
biden_polarity_afinn <- biden_polarity_plot %>%
# Inner join to lexicon
inner_join(afinn, by = c("term" = "word")) %>%
# Count the sentiment scores
count(value, index) %>%
# Spread the sentiment into positive and negative columns
spread(value, n, fill = 0) %>%
# Add polarity column
mutate(polarity = positive - negative)
ggplot(biden_polarity, aes(x = index, y = polarity)) +
# Add a smooth trend curve
geom_smooth()
# Combine. Use bind_rows() to combine ag_afinn to oz_afinn. Set the .id argument to "book" to create a new column with the name of each book.
all_df_afinn <- bind_rows(biden = biden_afinn_words,
trump = trump_afinn_words,
.id = "Tweets")
# Plot value, filled by Tweets
ggplot(all_df_afinn, aes(value, fill = Tweets)) +
# Set the alpha transparency to 0.3.
geom_density(alpha = 0.3) +
theme_gdocs() +
ggtitle("AFINN Score Densities")
rmarkdown::render("trump_biden_presentation.Rmd")
("trump_biden_presentation.Rmd")
rmarkdown::render("trump_biden_presentation.Rmd")
rmarkdown::render("trump_biden_presentation.Rmd")
# Plot value, filled by Tweets
ggplot(all_df_afinn, aes(value, fill = Tweets)) +
# Set the alpha transparency to 0.3.
geom_density(alpha = 0.3) +
theme_gdocs() +
ggtitle("AFINN Score Densities")
# Plot total_value vs. line
ggplot(biden_afinn_agg_pl, aes(x = index, y = total_value)) +
# Add a smooth trend curve
geom_smooth() + ggtitle("Plot of AFINN by Each Tweet") +
xlab("Tweet Number") +
ylab("AFINN Value")
# Get Bing lexicon
bing <- get_sentiments("bing")
biden_bing_words <- inner_join(biden_tidy, bing, by = c("term" = "word"))
# Get counts by sentiment
biden_bing_words %>%
count(sentiment)
# Create a column index, equal to as.numeric() applied to document. This occurs within mutate() in the tidyverse.
biden_bing_words <- biden_bing_words %>%
# Set index to numeric document
mutate(index = as.numeric(document))
head(biden_bing_words,10)
# Create ag_count by forwarding ag_bing_words to count(), passing in sentiment, index.
biden_count <- biden_bing_words %>%
# Count by sentiment, index
count(sentiment, index)
# Generate moby_spread by piping ag_count to spread() which contains sentiment, n, and fill = 0.
biden_spread <- biden_count %>%
# Spread sentiments
spread(sentiment, n, fill = 0)
# exclude variables v1, v2, v3
biden_polarity_plot <- biden_bing_words %>%
select(term, count, index)
str(biden_polarity_plot)
biden_polarity <- biden_polarity_plot %>%
# Inner join to lexicon
inner_join(bing, by = c("term" = "word")) %>%
# Count the sentiment scores
count(sentiment, index) %>%
# Spread the sentiment into positive and negative columns
spread(sentiment, n, fill = 0) %>%
# Add polarity column
mutate(polarity = positive - negative)
biden_polarity
biden_polarity
# Get Bing lexicon
bing <- get_sentiments("bing")
head(bing)
# Join text to lexicon (bing)
str(biden_tidy)
biden_bing_words <- inner_join(biden_tidy, bing, by = c("term" = "word"))
# Examine
biden_bing_words
# Get counts by sentiment
biden_bing_words %>%
count(sentiment)
# Create a column index, equal to as.numeric() applied to document. This occurs within mutate() in the tidyverse.
biden_bing_words <- biden_bing_words %>%
# Set index to numeric document
mutate(index = as.numeric(document))
head(biden_bing_words,10)
# Create ag_count by forwarding ag_bing_words to count(), passing in sentiment, index.
biden_count <- biden_bing_words %>%
# Count by sentiment, index
count(sentiment, index)
biden_count
# Generate moby_spread by piping ag_count to spread() which contains sentiment, n, and fill = 0.
biden_spread <- biden_count %>%
# Spread sentiments
spread(sentiment, n, fill = 0)
# Review the spread data
biden_spread
head(biden_bing_words)
# exclude variables v1, v2, v3
biden_polarity_plot <- biden_bing_words %>%
select(term, count, index)
str(biden_polarity_plot)
biden_polarity <- biden_polarity_plot %>%
# Inner join to lexicon
inner_join(bing, by = c("term" = "word")) %>%
# Count the sentiment scores
count(sentiment, index) %>%
# Spread the sentiment into positive and negative columns
spread(sentiment, n, fill = 0) %>%
# Add polarity column
mutate(polarity = positive - negative)
biden_polarity
# Plot polarity vs. index
ggplot(biden_polarity, aes(x = index, y = polarity)) +
# Add a smooth trend curve
geom_smooth()
# Plot polarity vs. index
ggplot(biden_polarity, aes(x = index, y = polarity)) +
# Add a smooth trend curve
geom_smooth()
# Plot polarity vs. index
ggplot(biden_polarity, aes(x = index, y = polarity)) +
# Add a smooth trend curve
geom_smooth()
biden_tidy1 <- biden_tidy %>%
filter(!term %in% c("the", "get", "just", "can", "don't", "stil", "it's", "its", "this", "and", "'re", "I'm", "many", "i'm","'re ", " 're", "cant", "can't", "i'm ", "i'll", "can't", "it's ", "but", "let"))
# Start with the tidied Twitter data
biden_tidy_dtm <- biden_tidy1 %>%
# Count each word used in each tweet
count(term, document) %>%
# Use the word counts by tweet to create a DTM
cast_sparse(document, term, n)
# Coerce dtm_twitter into a matrix called matrix_twitter
matrix_biden_topic <- as.matrix(biden_tidy_dtm)
# Run an LDA with 2 topics and a Gibbs sampler
lda_out_biden5 <- LDA(
biden_tidy_dtm,
k = 5,
method = "Gibbs",
control = list(seed = 1200)
)
# Tidy the matrix of word probabilities
lda_topics_biden5 <- lda_out_biden5 %>%
tidy(matrix = "beta")
# Arrange the topics by word probabilities in descending order___ %>%
lda_topics_biden5 %>%
arrange(desc(beta))
biden_documents <- tidy(lda_out_biden5, matrix = "gamma") %>% arrange(desc(gamma))
# Select the top 15 terms by topic and reorder term
word_probs_biden5 <- lda_topics_biden5 %>%
group_by(topic) %>%
top_n(20, beta) %>%
ungroup() %>%
mutate(term5 = fct_reorder(term, beta))
word_probs_biden5$topic <- factor(word_probs_biden5$topic, levels = c("1", "2", "3", "4", "5"), labels = c("Nation", "Trump Presidency", "Campain and Election", "Health Care", "Biden Presidency"))
ggplot(
word_probs_biden5,
aes(term5, beta, fill = as.factor(topic))
) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
coord_flip()
rmarkdown::render("trump_biden_presentation.Rmd")
